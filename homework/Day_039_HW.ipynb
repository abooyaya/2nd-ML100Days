{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境\n",
    "\n",
    "L1是 alpha常數*sum(abs(weights))\n",
    "L2是 alpha常數*sum(weights**2)\n",
    "\n",
    "alpha值越大，懲罰越明顯\n",
    "\n",
    "L1, L2 都是為了做Regularization(normalization, 避免overfitting)\n",
    "\n",
    "LASSO 是 Linear Regression + L1\n",
    "Ridge 是 Linear Regression + L2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "\n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "回答：可以\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 什麼是 Multi-Collinearity？\n",
    "\n",
    "Multi-Collinearity：多元共線性是指多元迴歸分析中，自變項之間有相關存在的一種現象。\n",
    "\n",
    "是一種程度的問題(degree of matters)，而不是全有或全無(all or none)的狀態，是一種程度的問題(degree of matters)，而不是全有或全無(all or none)的狀態。\n",
    "\n",
    "多元共線性若是達嚴重的程度時，會對多元迴歸分析造成下列的不良影響： 　　\n",
    "1.膨脹最小平方法(least squares)估計參數值的變異數和共變數，使得迴歸係數的估計值變得很不精確；\n",
    "2.膨脹迴歸係數估計值的相關係數；\n",
    "3.膨脹預測值的變異數，但對預測能力不影響；\n",
    "4.造成解釋迴歸係數及其信賴區間估計之困難；\n",
    "5.造成整體模式的考驗達顯著，但各別迴歸係數之考驗不顯著的矛盾現象和解釋上之困擾；\n",
    "6.造成迴歸係數的正負號與所期望者相反的衝突現象，這是由於自變項間之壓抑效果(suppress effect)造成的。\n",
    "\n",
    "一個比較簡單的診斷方法是察看自變項間的相關係數矩陣，看看該矩陣中是否有元素值（即自變項兩兩之間的相關係數值）是大於.90以上者，若有，即表示該二變項互為多元共線性變項，並認為該迴歸分析中有嚴重的多元共線性問題存在。另一個比較正式、客觀的診斷法，則為使用第j個自變項的「變異數膨脹因子」(variance inflation factor)作為判斷的指標，凡變異數膨脹因子指標值大於10者，即表示第j個自變項是一個多元共線性變項。在一般的迴歸分析中，針對這種多元共線性問題，有些統計學家會建議將多元共線性變項予以刪除，不納入迴歸方程式中。\n",
    "\n",
    "但避免多元共線性問題所造成困擾的最佳解決方法，不是刪除該具有多元共線性變項，而是使用所謂的「偏差迴歸分析」(biased regression analysis, BRA)。\n",
    "\n",
    "其中以「山脊型迴歸」(ridge regression)最受到學者們的重視和使用；除此之外，尚有「主成分迴歸」(principal component regression)、「潛在根迴歸」(latent root regression)、「貝氏法迴歸」(Baysean regression)、「遞縮式迴歸」(shrinkage regression)等，不過這些偏差迴歸分析法所獲得的迴歸係數值都是「有偏差的」(biased)，亦即這些迴歸係數的期望值不等於母群體的迴歸係數值，所以稱作偏差迴歸係數估計值，而本補救多元共線性問題的方法即稱作偏差迴歸分析法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考文章 - Linear Regression:\n",
    "https://blog.csdn.net/daunxx/article/details/51556677\n",
    "\n",
    "最小二乘法（Ordinary Least Squares）\n",
    "线性回归（Linear Regression）通过寻找合适的ω={w0,w1,w2,...,wn}ω={w0,w1,w2,...,wn}，使得观测样本集合XX（这里观测样本集合XX在很多的外文书籍中比如RPML，叫做设计矩阵（design matrix）），和目标输出yy之间的残差平方和最小(residual sum of squares)，从数学上来说，其的目标问题或者叫做损失函数是： \n",
    "J(w)=minw‖Xw−y‖2\n",
    "J(w)=minw‖Xw−y‖2\n",
    "\n",
    "注意：最小二乘法对ω的估计，是基于模型中变量之间相互独立的基本假设的，即输入向量x中的任意两项xi和xj之间是相互独立的。\n",
    "\n",
    "如果输入矩阵X中存在线性相关或者近似线性相关的列，那么输入矩阵X就会变成或者近似变成奇异矩阵（singular matrix）。这是一种病态矩阵，矩阵中任何一个元素发生一点变动，整个矩阵的行列式的值和逆矩阵都会发生巨大变化。\n",
    "\n",
    "这将导致最小二乘法对观测数据的随机误差极为敏感，进而使得最后的线性模型产生非常大的方差，这个在数学上称为多重共线性（multicollinearity）。在实际数据中，变量之间的多重共线性是一个非常普遍的现象，其产生机理及相关解决方案在“特征选择和评估”中有介绍。\n",
    "\n",
    "\n",
    "singular matrix:矩陣有逆矩陣者（參見inverse matrix）稱為可逆矩陣(invertible matrix)，亦即非奇異矩陣(nonsingular matrix)，反之，無逆矩陣存在者，稱為奇異矩陣。奇異矩陣的行列值恆為零，亦即各行（列）不為獨立。\n",
    "\n",
    "nonsingular matrix: 方陣的行列值(determinant)為零時稱為奇異(singular)；反之，行列值不為零時稱為非奇異(nonsingular)。非奇異方陣亦稱為可逆方陣(invertible matrix)，因為恆有唯一的逆陣(inverse)存在。非奇異方陣中，各行與各列均為線性獨立，是一個滿秩(rank)的方陣。\n",
    "\n",
    "ill-conditioned matrix:求解方程组时如果对数据进行较小的扰动，则得出的结果具有很大波动，这样的矩阵称为病态矩阵。\n",
    "在求解任何反问题的过程中通常会遇到病态矩阵问题，而且病态矩阵问题还未有很好的解决方法，尤其是长方形、大型矩阵。目前主要有Tikhonov、奇异值截断、奇异值修正、迭代法等方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考文章 - Ridge Regression:\n",
    "https://blog.csdn.net/daunxx/article/details/51578787\n",
    "\n",
    "当使用最小二乘法计算线性回归模型参数的时候，如果数据集合矩阵（也叫做设计矩阵(design matrix)）XX，存在多重共线性，那么最小二乘法对输入变量中的噪声非常的敏感，其解会极为不稳定。为了解决这个问题，就有了这一节脊回归（Ridge Regression ）。\n",
    "\n",
    "当设计矩阵XX存在多重共线性的时候（数学上称为病态矩阵），最小二乘法求得的参数ww在数值上会非常的大，而一般的线性回归其模型是 y=wTxy=wTx ，显然，就是因为ww在数值上非常的大，所以，如果输入变量xx有一个微小的变动，其反应在输出结果上也会变得非常大，这就是对输入变量总的噪声非常敏感的原因。\n",
    "\n",
    "如果能限制参数ww的增长，使ww不会变得特别大，那么模型对输入ww中噪声的敏感度就会降低。这就是脊回归和套索回归（Ridge Regression and Lasso Regrission）的基本思想。\n",
    "\n",
    "为了限制模型参数ww的数值大小，就在模型原来的目标函数上加上一个惩罚项，这个过程叫做正则化（Regularization）。\n",
    "\n",
    "如果惩罚项是参数的l2范数，就是脊回归(Ridge Regression)\n",
    "\n",
    "如果惩罚项是参数的l1范数，就是套索回归（Lasso Regrission）\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
