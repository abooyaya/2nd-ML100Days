{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "\n",
    "\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "回答：應該可以，如果透過有\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n",
    "回答：兩種都可以\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Index: Gini impurity(It's not Gini coefficient.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/) 心得：\n",
    "\n",
    "1. decision tree algorithm can be used for solving regression and classification problems (Decision Tree可以解決 regression 和 classification兩種問題)\n",
    "2. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label. (internal node是屬性, leaf node是 label)\n",
    "\n",
    "3. The below are the some of the assumptions we make while using Decision tree (以下是一些使用Desicion Tree時的假設):<br>\n",
    "3.1. At the beginning, the whole training set is considered as the root.(一開始，把整個training set 放到root)<br>\n",
    "3.2. Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model. (feature要是類別型的，如果不是，那要先作轉換)<br>\n",
    "3.3. Records are distributed recursively on the basis of attribute values.() <br>\n",
    "3.4. Order to placing attributes as root or internal node of the tree is done by using some statistical approach. <br>\n",
    "\n",
    "4. Decision Trees follow Sum of Product (SOP) representation. The Sum of product(SOP) is also known as Disjunctive Normal Form. For a class, every branch from the root of the tree to a leaf node having the same class is a conjunction(product) of values, different branches ending in that class form a disjunction(sum).\n",
    "\n",
    "5. The primary challenge in the decision tree implementation is to identify which attributes do we need to consider as the root node and each level. Handling this is know the attributes selection. \n",
    "\n",
    "6. The popular attribute selection measures:<br>\n",
    "6.1. Information gain<br>\n",
    "6.2. Gini index<br>\n",
    "\n",
    "7. While using information Gain as a criterion, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda) 心得\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
